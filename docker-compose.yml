version: '3'
services:
  backend:
    build: ./backend
    restart: unless-stopped
    environment:
      - MODEL_SIZE=3B  # Can be changed to 3B
      - DEVICE_MAP=auto  # Changed back to auto for GPU usage
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
    volumes:
      - huggingface_cache:/app/.cache/huggingface  # Cache for models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  frontend:
    build: ./frontend
    restart: unless-stopped
    depends_on:
      - backend

  nginx:
    build: ./nginx
    restart: unless-stopped
    ports:
      - "8443:443"  # Map external port 8443 to internal 443
    volumes:
      - ./certs:/etc/nginx/certs:ro
    depends_on:
      - frontend
      - backend

volumes:
  huggingface_cache:  # Named volume for model cache